{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, I started look into the possibility of replacing the PCA part of platform pipeline with an autoencoder approach in order to leverage gpu computing for potential speed-up. In this notebook I will look into two different kinds of autoencoder, i.e. Variational autoencoder and Convolutional autoencoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was entirely run on a google cloud compute engine VM instance, which has a Nvidia Tesla P-100 GPU with 16 GB memory. Other important dependencies are as follow:\n",
    "\n",
    "1. **Tensorflow**: 1.5\n",
    "2. **CUDA**: 9.0\n",
    "3. **cuDNN**: 7.0.4\n",
    "4. **numpy**: 1.14.0\n",
    "5. **sklearn**: 0.19.1\n",
    "6. **xgboost**: 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yimingliuno1/anaconda3/envs/TF/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from AutoNets import Autoencoder,Transformer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import time\n",
    "import h5py\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    with h5py.File(file_path,'r') as f:\n",
    "        data=f.get('dataset_1')\n",
    "        Array=np.array(data)\n",
    "    return Array\n",
    "\n",
    "def pca(n_components,target):\n",
    "    model=PCA(n_components=n_components)\n",
    "    model.fit(target)\n",
    "    PCs=model.transform(target)\n",
    "    return PCs\n",
    "\n",
    "def min_max_scale(data):\n",
    "    Max=data.max()\n",
    "    Min=data.min()\n",
    "    return (data-Min)/(Max-Min)\n",
    "\n",
    "\n",
    "def data_split(data,label,test_ratio=0.2):\n",
    "    sss=StratifiedShuffleSplit(n_splits=1,test_size=test_ratio,random_state=0)\n",
    "    for train_index,test_index in sss.split(data,label):\n",
    "        return train_index,test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data used in this analysis will be the same as the previous one. However, I made two modifications regarding the data used. Firstly only 1000 samples will be used to avoid memory issue. Secondly only layer activations from the layer named block2_pool, which is the second pooling layer in the network, will be used, since the bottleneck of the current pipeline resides on the slow dimensionality reduction on the very-high dimensional earlier layer activations. Therefore, input data in below experiment will have shape (1000,401408) or (1000,56,56,128) for convolutional case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_features=read_data('block2_poll_train_features.h5')# load block2_pool_train_features\n",
    "vgg_features=vgg_features[:1000,:]# Take a subset\n",
    "label=np.load('imagenet_train_label.pk')\n",
    "label=label[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time:23.85962963104248 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time the speed of PCA\n",
    "start=time.time()\n",
    "PCs_block2_pool=pca(128,vgg_features)\n",
    "print('Run Time:{} seconds'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_index,test_index=data_split(PCs_block2_pool,label)# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(PCs_block2_pool,open('PCs_block2_pool.pk','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a xgboost classifier and fit it with train data\n",
    "model=xgb.XGBClassifier()\n",
    "model.fit(PCs_block2_pool[train_index,:],label[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yimingliuno1/anaconda3/envs/TF/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.355"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions using test data and check the accuracy\n",
    "predictions=model.predict(PCs_block2_pool[test_index,:])\n",
    "accuracy_score(label[test_index],predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts**: PCA performance is very constistent with its performance in previous experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epchs: 1/10 Loss: 376449.65625\n",
      "Number of epchs: 2/10 Loss: 383916.1875\n",
      "Number of epchs: 3/10 Loss: 358984.78125\n",
      "Number of epchs: 4/10 Loss: 346379.71875\n",
      "Number of epchs: 5/10 Loss: 348259.75\n",
      "Number of epchs: 6/10 Loss: 347537.09375\n",
      "Number of epchs: 7/10 Loss: 342321.96875\n",
      "Number of epchs: 8/10 Loss: 336006.40625\n",
      "Number of epchs: 9/10 Loss: 329707.8125\n",
      "Number of epchs: 10/10 Loss: 324625.03125\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/auto\n",
      "Model trained and saved\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/auto\n",
      "Run Time:52.07313561439514 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time the speed of Variational Autoencoder\n",
    "start=time.time()\n",
    "# build an one-hidden layer variational autoencoder\n",
    "auto=Autoencoder(401408,1,1,[128],[401408],mode='Variation',activation=tf.nn.elu)\n",
    "vgg_features=auto.scale_data(vgg_features,method='Normal')\n",
    "model=auto.build(lr=1e-4)\n",
    "auto.train(vgg_features,10,32,model,return_pc=False)\n",
    "model=Transformer(vgg_features,mode='Variation')\n",
    "encodings,_=model.transform()\n",
    "print('Run Time:{} seconds'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_index,test_index=data_split(encodings,label)# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a xgboost classifier and fit it with train data\n",
    "model2=xgb.XGBClassifier()\n",
    "model2.fit(encodings[train_index,:],label[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yimingliuno1/anaconda3/envs/TF/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions using test data and check the accuracy\n",
    "predictions=model2.predict(encodings[test_index,:])\n",
    "accuracy_score(label[test_index],predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts**: In terms of classification accuracy, variational autoencoder is the best approach among all approaches I have ever tried. However, it also employes largest number of parameters, which significantly slows down the saving process of the trained model, which in turn slows down the process of obtaining the desired encodings, as model is firstly saved and the inference is performed in my implementation. A way to work around this issue would be to perform inference within the same tensorflow session as the training. Thus by sacrificing more GPU memory, we would eliminate the need to save the model, and this method will be illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder with in session inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epchs: 1/10 Loss: 418679.375\n",
      "Number of epchs: 2/10 Loss: 401375.78125\n",
      "Number of epchs: 3/10 Loss: 384766.15625\n",
      "Number of epchs: 4/10 Loss: 373281.15625\n",
      "Number of epchs: 5/10 Loss: 369464.46875\n",
      "Number of epchs: 6/10 Loss: 356834.71875\n",
      "Number of epchs: 7/10 Loss: 351872.34375\n",
      "Number of epchs: 8/10 Loss: 349648.375\n",
      "Number of epchs: 9/10 Loss: 354882.8125\n",
      "Number of epchs: 10/10 Loss: 344756.40625\n",
      "Run Time:20.920249700546265 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time the speed of Variational Autoencoder\n",
    "start=time.time()\n",
    "auto=Autoencoder(401408,1,1,[128],[401408],mode='Variation',activation=tf.nn.elu)\n",
    "vgg_features=auto.scale_data(vgg_features,method='Normal')\n",
    "model=auto.build(lr=1e-4)\n",
    "encodings=auto.train(vgg_features,10,32,model,return_pc=True)\n",
    "print('Run Time:{} seconds'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_index,test_index=data_split(encodings,label)# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a xgboost classifier and fit it with train data\n",
    "model2=xgb.XGBClassifier()\n",
    "model2.fit(encodings[train_index,:],label[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yimingliuno1/anaconda3/envs/TF/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions using test data and check the accuracy\n",
    "predictions=model2.predict(encodings[test_index,:])\n",
    "accuracy_score(label[test_index],predictions)Stacked autoencoder without nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts**: By eliminating saving the network all together, considerable amount of time has been saved, and VAE in this case, is actually slightly faster than the PCA approach. However, doing this will sacrifice more GPU memory, which might casuse out of memory error in certain situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_features=vgg_features.reshape(-1,56,56,128)# convert data to 3-dimensional tensorStacked autoencoder without nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epchs: 1/10 Loss: 385214.34375\n",
      "Number of epchs: 2/10 Loss: 370110.21875\n",
      "Number of epchs: 3/10 Loss: 362324.84375\n",
      "Number of epchs: 4/10 Loss: 357767.03125\n",
      "Number of epchs: 5/10 Loss: 353236.9375\n",
      "Number of epchs: 6/10 Loss: 343601.6875\n",
      "Number of epchs: 7/10 Loss: 337408.65625\n",
      "Number of epchs: 8/10 Loss: 333473.84375\n",
      "Number of epchs: 9/10 Loss: 331280.0625\n",
      "Number of epchs: 10/10 Loss: 330185.03125\n",
      "Model trained and saved\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/auto\n",
      "Run Time:12.177278757095337 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time the speed of Convolutional Autoencoder\n",
    "start=time.time()\n",
    "auto=Autoencoder([56,56,128],3,3,[64,32,4],[32,64,128],ksize=3,stride=2,mode='Convolution',activation=tf.nn.relu)\n",
    "model=auto.build(lr=1e-3)\n",
    "auto.train(vgg_features,10,32,model,return_pc=False)\n",
    "model=Transformer(vgg_features,mode='Convolution')\n",
    "encodings,_=model.transform()\n",
    "print('Run Time:{} seconds'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encodings=np.reshape(encodings,[1000,7*7*4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_index,test_index=data_split(encodings,label)# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a xgboost classifier and fit it with train data\n",
    "model2=xgb.XGBClassifier()\n",
    "model2.fit(encodings[train_index,:],label[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yimingliuno1/anaconda3/envs/TF/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions using test data and check the accuracy\n",
    "predictions=model2.predict(encodings[test_index,:])\n",
    "accuracy_score(label[test_index],predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Thoughts **: convolutional autoencoder has a sizable speed advantage over PCA. However, during my experiment, it seems to be hard to reach the same level of encoding quality as PCA, measured by classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above experiments, we can see variational autoencoder offers the encodings with highest quality, but the speed boost seems to be less obvious. On the other hand, convolutional autoencoder although offers a sizable speed boost, quality of its encodings is not very optimal.A summary table of above experiments is presented below.\n",
    "\n",
    "\n",
    "\n",
    "**Performance on second Pooling layer activations**\n",
    "\n",
    "| Models | Time|Classification accuracy|\n",
    "|------|------|\n",
    "|   PCA| 23.86 s|0.355|\n",
    "|   VAE| 52.07 s|0.4397|\n",
    "|   VAE with in-session-inference| 20.92 s|0.37|\n",
    "|   Convolutional autoencoder| 12.17 s|0.26|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF]",
   "language": "python",
   "name": "conda-env-TF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
